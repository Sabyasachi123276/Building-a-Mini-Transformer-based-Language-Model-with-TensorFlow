{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to LLMs & Transformers**\n",
        "\n",
        "(i) **LLMs:** Large Language Models (LLMs) are deep learning models trained on vast text corpora to understand and generate human-like language. They excel at tasks like text completion, translation, summarization, and question answering.\n",
        "\n",
        "(ii) **Transformers:** At the core of most LLMs is the Transformer architecture, introduced in 2017, which uses self-attention mechanisms to process input sequences in parallel rather than sequentially. This allows Transformers to capture complex dependencies and long-range context more efficiently than earlier RNNs or LSTMs. Modern LLMs like Google DeepMind’s Gemini, OpenAI's GPT (Generative Pre-trained Transformer), Google Research's BERT (Bidirectional Encoder Representations from Transformers), and Google Research and Brain Team's T5 (Text-To-Text Transfer Transformer) are scaled-up versions of the Transformer, leveraging massive data and computation. Understanding Transformers is key to grasping how LLMs work and how they can be customized for specific language tasks."
      ],
      "metadata": {
        "id": "e0zDiwP76pum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationships between LLMs, Transformers, and Generative AI**\n",
        "\n",
        "Large Language Models (LLMs), Transformers, and Generative AI are deeply interconnected components of modern artificial intelligence.\n",
        "\n",
        "LLMs are a subset of Generative AI systems specifically designed to understand and produce human language. These models are typically built upon the Transformer architecture, which revolutionized NLP by introducing self-attention mechanisms that enable efficient processing of long text sequences. Transformers form the computational backbone of most state-of-the-art LLMs like Google DeepMind’s Gemini, OpenAI's GPT, Google Research's BERT, and Google Research and Brain Team's T5.\n",
        "\n",
        "Generative AI, a broader category, includes models that create new content—text, images, code, music—based on learned patterns. LLMs are a key pillar of Generative AI in the textual domain. By using Transformers, LLMs can generate coherent, contextually relevant language outputs, making them capable of creative tasks like storytelling, summarization, and dialogue. In essence, Transformers enable LLMs, and LLMs serve as the language engine of Generative AI applications."
      ],
      "metadata": {
        "id": "iyT-7h0b6wEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are the problems associated with Keras's high-level abstractions, such as the Sequential model or model.fit() pipeline?**\n",
        "\n",
        "Keras's high-level abstractions like Sequential and model.fit() offer simplicity and speed for prototyping but come with several limitations, especially when building complex or customized models. Here are the main problems:\n",
        "\n",
        "**1. Limited Architectural Flexibility (Sequential)**\n",
        "\n",
        "**(i)** Sequential assumes a linear stack of layers.\n",
        "\n",
        "**(ii)** Difficult or impossible to build models with:\n",
        "\n",
        "(a) Multiple inputs or outputs.\n",
        "\n",
        "(b) Skip connections (e.g., in ResNets).\n",
        "\n",
        "(c) Shared layers across different paths.\n",
        "\n",
        "(d) Dynamic branching or conditional logic.\n",
        "\n",
        "\n",
        "**2. Less Control Over Training Logic (model.fit)**\n",
        "\n",
        "**(i)** model.fit() is a black-box training loop.\n",
        "\n",
        "**(ii)** Hard to:\n",
        "\n",
        "(a) Implement custom loss functions that depend on intermediate layers.\n",
        "\n",
        "(b) Add per-batch logic (e.g., curriculum learning, adaptive loss scaling).\n",
        "\n",
        "(c) Handle non-standard data flows (e.g., RL or self-supervised learning).\n",
        "\n",
        "(d) Manage variable-length sequences without padding (e.g., NLP tasks).\n",
        "\n",
        "\n",
        "**3. Difficulty with Debugging and Advanced Logging**\n",
        "\n",
        "**(i)** Less transparent than a custom training loop.\n",
        "\n",
        "**(ii)** Debugging issues like gradient explosions, NaNs, or unexpected outputs can be tricky.\n",
        "\n",
        "**(iii)** Custom logging or interaction with intermediate tensors (e.g., attention maps) is limited.\n",
        "\n",
        "\n",
        "**4. Inflexible for Custom Metrics or Loss Aggregation**\n",
        "\n",
        "**(i)** model.fit() expects metrics/losses in a specific format.\n",
        "\n",
        "**(ii)** Aggregating complex multi-output or multi-task losses can be cumbersome.\n",
        "\n",
        "**(iii)** Limited support for online evaluation or sample-level feedback.\n",
        "\n",
        "\n",
        "**5. Performance Trade-offs**\n",
        "\n",
        "**(i)** Some optimizations (e.g., mixed precision, gradient accumulation, custom update rules) are harder to integrate into model.fit().\n",
        "\n",
        "**(ii)** Custom training loops using tf.GradientTape allow more fine-grained performance tuning.\n",
        "\n",
        "\n",
        "**When to Avoid High-Level Abstractions:**\n",
        "\n",
        "**(i)** Multi-modal models (e.g., images + text).\n",
        "\n",
        "**(ii)** Graph neural networks.\n",
        "\n",
        "**(iii)** Custom training workflows (e.g., contrastive learning, adversarial training).\n",
        "\n",
        "**(iv)** Reinforcement learning or decision trees mixed with DL."
      ],
      "metadata": {
        "id": "cluxtyLZFN0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement:** Design and implement a character-level language model using a simplified Transformer architecture built with low-level TensorFlow (not Keras high-level API), to learn and generate English-like text from a small custom dataset."
      ],
      "metadata": {
        "id": "f4Ci5O0g62mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** This hands-on exercise focuses on building a custom character-level language model using the Transformer architecture, implemented directly with low-level TensorFlow primitives. Without relying on Keras's high-level abstractions like Sequential or Model.fit pipelines, participants construct embedding layers, multi-head attention, normalization, and dense layers manually to better understand the core components of a Transformer.\n",
        "\n",
        "The model is trained on a small sample corpus and generates text character-by-character. The goal is to help learners grasp the essential concepts behind Transformer models in natural language processing and how these models perform autoregressive generation."
      ],
      "metadata": {
        "id": "9WamNAIO657n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GZgcTLA46Ysk",
        "outputId": "dc684cff-1b5b-4e61-9b23-d1b3e8b7452f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1535 - loss: 3.1255\n",
            "Epoch 2/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4079 - loss: 1.9976\n",
            "Epoch 3/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4430 - loss: 1.6803\n",
            "Epoch 4/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3736 - loss: 1.6314\n",
            "Epoch 5/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4303 - loss: 1.5085\n",
            "Epoch 6/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5237 - loss: 1.3373\n",
            "Epoch 7/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4708 - loss: 1.4518\n",
            "Epoch 8/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4970 - loss: 1.4278\n",
            "Epoch 9/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5377 - loss: 1.3087\n",
            "Epoch 10/10\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6062 - loss: 1.1491\n",
            "the \n"
          ]
        }
      ],
      "source": [
        "# 1. Setup\n",
        "!pip install tensorflow                                                                                         #Imports TensorFlow.\n",
        "import tensorflow as tf                                                                                         #Imports TensorFlow.\n",
        "import numpy as np                                                                                              #Imports numpy.\n",
        "import re                                                                                                       #Imports Regular Expression and is used for pattern matching and searching within strings. (Regular Expression)\n",
        "import string                                                                                                   #Imports string to provide useful string constants and tools.\n",
        "\n",
        "# 2. Sample Dataset\n",
        "text = \"\"\"The sun sets in the west. The stars shine in the night. The moon rises with grace.\"\"\"                  #A small text corpus is defined and converted to lowercase for uniformity (reduces character variation).\n",
        "text = text.lower()                                                                                              ## Convert all characters to lowercase for consistency.\n",
        "\n",
        "# 3. Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)                                               # Create a character-level tokenizer.\n",
        "tokenizer.fit_on_texts([text])                                                                                   #Learn the character vocabulary from the text.\n",
        "total_chars = len(tokenizer.word_index) + 1                                                                      #Gets the total number of unique characters + 1 (for padding).\n",
        "\n",
        "sequences = []                                                                                                   #List to hold input sequences\n",
        "for i in range(1, len(text)):                                                                                    # Create all prefix sequences from text, e.g., \"t\", \"th\", \"the\", ...\n",
        "    input_seq = text[:i]                                                                                         # For every character position i, creates an input sequence from the start of the text up to i.\n",
        "    sequences.append(tokenizer.texts_to_sequences([input_seq])[0])                                               # Convert to integer sequence.[0] → gets the sequence of the first (and only) string and as a consequence a list of sequences, we are getting one per input string.\n",
        "\n",
        "max_len = max(len(x) for x in sequences)                                                                          # Find max sequence length\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='pre')               # Pad all sequences to the same length\n",
        "sequences = np.array(sequences)                                                                                   # Convert list to NumPy array\n",
        "X, y = sequences[:, :-1], sequences[:, -1]                                                                        # Split into input and target (last character)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_chars)                                                     # One-hot encode the target\n",
        "\n",
        "# 4. Build a Mini Transformer Block\n",
        "class MiniTransformer(tf.keras.Model):                                                                            # Define a custom Transformer-based model\n",
        "    def __init__(self, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, 64)                                                # Learn character embeddings\n",
        "        self.pos_encoding = tf.keras.layers.Embedding(input_dim=max_len, output_dim=64)                           # Positional encoding\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)                                    # Multi-head self-attention\n",
        "        self.norm = tf.keras.layers.LayerNormalization()                                                          # Layer normalization\n",
        "        self.ff = tf.keras.Sequential([                                                                           # Feed-forward network\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64)\n",
        "        ])\n",
        "        self.out = tf.keras.layers.Dense(vocab_size, activation='softmax')                                        # Output layer for next character prediction\n",
        "\n",
        "    def call(self, x):\n",
        "     positions = tf.range(start=0, limit=tf.shape(x)[-1], delta=1)                                                # Create position indices\n",
        "     x_embed = self.embedding(x) + tf.cast(self.pos_encoding(positions), dtype=tf.float32)                        # Add embeddings + positions\n",
        "\n",
        "     # Self-attention block\n",
        "     attn_output = self.att(x_embed, x_embed)                           # Apply self-attention\n",
        "     x1 = self.norm(x_embed + attn_output)                              # Residual connection + norm\n",
        "\n",
        "     # Feed-forward block\n",
        "     ff_output = self.ff(x1)                                            # Apply feed-forward network\n",
        "     x2 = self.norm(x1 + ff_output)                                     # Another residual connection + norm\n",
        "\n",
        "     return self.out(x2[:, -1, :])                                      # Only return last token prediction\n",
        "\n",
        "\n",
        "model = MiniTransformer(vocab_size=total_chars, max_len=max_len-1)                                                  # Instantiate the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])                              # Compile with optimizer & loss\n",
        "model.summary()                                                                                                     # Print model architecture\n",
        "\n",
        "# 5. Train Model (quick demo with few epochs)\n",
        "model.fit(X, y, epochs=10, batch_size=2)                                                                            # Train model for 10 epochs using small batch size\n",
        "\n",
        "# 6. Text Generation\n",
        "def generate_text(model, start_str, length=100):                                                                    # Generate text of desired length\n",
        "    for _ in range(length):\n",
        "        token_list = tokenizer.texts_to_sequences([start_str])[0]                                                   # Convert input text to sequence\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_len-1, padding='pre')   #Pad to match input length\n",
        "        prediction = model.predict(token_list, verbose=0)                  # Predict next character\n",
        "        predicted_id = np.argmax(prediction[0][-1])                        # Get most likely next character\n",
        "        next_char = tokenizer.index_word.get(predicted_id, '')             # Convert ID back to character\n",
        "        start_str += next_char                                             # Append character to the string\n",
        "    return start_str                                                       # Return generated text\n",
        "\n",
        "# Example\n",
        "print(generate_text(model, \"the \", length=50))     #Generate 50 characters starting with \"the\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Novelty of the execution:** This implementation teaches foundational LLM concepts by building a fully functional Transformer from scratch using low-level TensorFlow APIs in a character-level setting, optimized for transparency and fast experimentation."
      ],
      "metadata": {
        "id": "tU6LPyfi7Cz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications:**\n",
        "\n",
        "**(i) Understanding Core LLM Concepts**\n",
        "\n",
        "Helps learners grasp how large language models (like Gemini) are built from the ground up using attention, embeddings, and residual connections.\n",
        "\n",
        "**(ii) Text Generation Systems**\n",
        "\n",
        "Forms the basis for applications like chatbots, story generators, autocomplete engines, or poetry generators—even if in a simplified form.\n",
        "\n",
        "**(iii) Language Modeling in Low-Resource Settings**\n",
        "\n",
        "Character-level modeling is useful for languages or domains without robust tokenizers or large datasets.\n",
        "\n",
        "**(iv) Educational & Research Prototypes**\n",
        "\n",
        "Excellent for academic courses, workshops, or sandbox experimentation to prototype new ideas before scaling up.\n",
        "\n",
        "**(v) Error Correction & Typo Prediction**\n",
        "\n",
        "Since it models sequences at the character level, it can be fine-tuned for spelling correction, auto-correction, or typo-aware input systems.\n",
        "\n",
        "**(vi) Custom Code Generation or Domain-Specific DSLs (Domain-Specific Languages)**\n",
        "\n",
        "Character-level models can be adapted for specialized domains like DNA sequences, source code, or markup languages."
      ],
      "metadata": {
        "id": "Jqc_E17n6-pJ"
      }
    }
  ]
}
